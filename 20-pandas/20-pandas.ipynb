{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises 20: pandas\n",
    "Let's have a look at the `pandas` library. This is basically a wrapper around numpy arrays, offering the `DataFrame` object, adding names to column and unique id to rows, allowing more explicit and condensed syntax for statistical analysis of data. Having row unique ids also allows automatic merging of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these exercises we will need `pandas`, `numpy` and `matplotlib`, as well as our favorite dataset, the diabetes disease progression. So let's import those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.datasets import load_diabetes\n",
    "try:\n",
    "    diabetes_np = load_diabetes(scaled=False)\n",
    "except:\n",
    "    diabetes_np = load_diabetes()\n",
    "    diabetes_np.data[:,1] = np.where(diabetes_np.data[:,1] > 0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use a book dataset with multiple tables (adapted from https://www.kaggle.com/datasets/oscaryezfeijo/my-library), to train merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "basedir = \".\"\n",
    "assets = os.path.join(basedir, \"assets\")\n",
    "if not os.path.isdir(assets):\n",
    "    print(f\"Could not find directory {os.path.abspath(assets)}\")\n",
    "    print(\"Please modify basedir above to point to the course's 20-pandas directory.\")\n",
    "else: \n",
    "    books = pandas.read_csv(os.path.join(assets, \"books.csv\"))\n",
    "    authors = pandas.read_csv(os.path.join(assets, \"authors.csv\"))\n",
    "    genres = pandas.read_csv(os.path.join(assets, \"genres.csv\"))\n",
    "    book_genre = pandas.read_csv(os.path.join(assets, \"book_genre.csv\"))\n",
    "    book_author = pandas.read_csv(os.path.join(assets, \"book_author.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 20.1: Creating a DataFrame\n",
    "Let's start by making a `DataFrame` from our diabetes data.\n",
    "\n",
    "The `DataFrame` can be instantiated with several arguments, you can notably pass the data and column names (look at the help for more information).\n",
    "* instantiate a `DataFrame` (we will use `diabetes` as variable name for it in these exercises) containing the diabetes data (`diabetes_np.data`) and corresponding column names (`diabetes_np.feature_names`)\n",
    "* Add a `\"Progression\"` column containing the target data. Use `diabetes.insert` for that purpose\n",
    "* Print out the first rows of `diabetes` (you can use the `head` method for that purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 20.2: Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.2.1: Basic information about a DataFrame\n",
    "A few useful attributes and methods to have a first look at the data are listed below. Try them out:\n",
    "* As in numpy, the `shape` attribute gives the shape (size in each dimension) of the data\n",
    "* The `columns` attribute returns the names of the columns in the `DataFrame`\n",
    "* The `info()` method returns some information on the `Series` in the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.2.2 Describe\n",
    "A summary of the data in a `DataFrame` can be obtained with the `describe` method. Try it out, assign the result to a variable `diabetes_description`, which we will use later in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.2.3 Sorting\n",
    "Sorting of the `DataFrame` can be done simply with the `sort_values` method, passing a column name as argument. Sort `diabetes` according to the `Progression` column and show the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 20.3: Slicing\n",
    "There are many ways to index and slice a `DataFrame`.\n",
    "### Exercise 20.3.1: direct slicing\n",
    "Simple indexing and slicing with the `[]` operator works as follows:\n",
    "* Passing a single element or a list of elements, looks for these column labels\n",
    "* Passing a slice (i.e. using the `from:to:step` syntax) slices the rows\n",
    "\n",
    "Try this out:\n",
    "* Get a single column of the dataframe using its name, for example retrieve the `\"bmi\"` column. \n",
    "* Now get the `\"Progression\"` and the `\"bmi\"` columns using a list of names\n",
    "* Finally get the first 10 rows.\n",
    "\n",
    "Notice that when you retrieve a single column you get a `Series` object back, in the other cases it's a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.3.2: The `iloc` method\n",
    "The `iloc` attribute allows normal indexing, as in numpy\n",
    "#### Exercise 20.3.2 a\n",
    "Use the `iloc` attribute to get\n",
    "* The first 5 rows\n",
    "* All columns except the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.3.3: The `loc` method\n",
    "More complex indexing using labels can be obtained with the `loc` attribute of the data frame. In that case, everything is always interpreted as labels, both for the rows and for the columns. Note that boundary indices are included in that case.\n",
    "\n",
    "Look at the help and then use indexing to access:\n",
    "* Only rows with labels between 1 and 5 (included)\n",
    "* all columns from `\"bmi\"` to `\"s3\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.3.3: Masks\n",
    "As in numpy, masks can be used for slicing. Try this out:\n",
    "* Use a mask to retrieve only the rows for which the `\"Progression\"` is larger than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.3.4: Reassignment and casting\n",
    "Currently all columns of our `DataFrame` have type `float64`. Cast `age` to `int32`.\n",
    "- You can use `astype` on a `Series` to cast it to a different type.\n",
    "- As with dictionaries, you can simply use \"indexing\" to assign a new value (`Series`) to a column\n",
    "- You can then use the `info` method on the `DataFrame` to check the new data types of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.3.5: Merging\n",
    "The books dataset consists of 5 tables (`DataFrame`): `books`, `authors`, `genres`, `book_author`, `book_genre`.  Books are identified by the `isbn`, authors by their `author_id` and genres by their `genre_id`.\n",
    "- Create a `DataFrame` with all books and their authors (merge `books` with `book_author` and `authors`)\n",
    "- Use it to list all books from author \"Pierre Bourdieu\"\n",
    "- Extend the `DataFrame` with the genres.\n",
    "- List all books from author \"Pierre Bourdieu\" of genre \"Nonfiction\" (you can use the `&` operator to combine two boolean arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 20.4 Basic statistics with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.4.1\n",
    "As for numpy arrays, standard statistic methods can be found as methods on the `DataFrame`. By default they are applied per column and missing values are skipped.\n",
    "* calculate the average (`mean`) for all columns in the `DataFrame`\n",
    "* calculate the standard deviation (`std`) for `Progression` and `bmi` (use slicing to only calculate the std deviation on these columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.4.2: Correlation matrix\n",
    "The correlation matrix is obtained with the `corr` method. Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.4.3: Broadcasting\n",
    "Again as in numpy, broadcasting is used when doing mathematical operations. For example define `diabetes_normalised` from `diabetes` by normalising each column (remove the `mean` and then divide by the `std`). This is easily done in one line using broadcasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 20.5: applying functions to Series or elements of a DataFrame\n",
    "Similarly to the `map` function from standard python, `DataFrame`s have an `apply` method:\n",
    "* `DataFrame.apply(func)` applies `func` to every `Series` in the `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.5.1: dimensional reduction\n",
    "When the function passed as argument to `apply` returns a single value for each column, we will have a dimensional reduction, called an aggregate operation. For example use the `apply` method to get the maximal value for each column (`Series`) in `diabetes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.5.2: no dimensional reduction\n",
    "Now use the `apply` method to take the square of every element in `diabetes`(use a `lambda` function). Notice that this time there is no dimensional reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.5.3: method name as argument\n",
    "The `apply` method can also take a string as argument, containing the name of a method that will be called on the `DataFrame`. Try that out taking the `mean` of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.5.4 (Supplementary): lists or dicts as arguments\n",
    "The `apply` method is very flexible and can also take as argument\n",
    "* a list of functions and strings (method names) that will be applied to the `DataFrame`\n",
    "* a dictionary with key:value pairs where keys are column names and values are functions or method names that will be applied on the corresponding column.\n",
    "\n",
    "#### Exercise 20.5.4 a\n",
    "Calculate the min, max and mean for every column of `diabetes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20.5.4 b\n",
    "Calculate the `mean` for the `Progression` column and the `min` and `max` for the `bmi` column of `diabetes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.5.5 (Supplementary)\n",
    "A more complex example is to calculate the correlation for every feature with the disease progression feature (using `apply`, a `lambda` function and of course `np.corrcoef`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 20.6: Grouping\n",
    "The `groupby` method allows to group the data in a `DataFrame` according to a feature.\n",
    "### Exercise 20.6.1: Grouping according to a feature\n",
    "We have one categorical feature in our dataset (`\"sex\"`). \n",
    "* group `diabetes` according to that feature and assign the result to a new variable `grouped`. \n",
    "* Have a look at that object (of class `DataFrameGroupBy`), it has a `groups` attribute, which is a dictionary of the values of `\"sex\"` corresponding to the two groups as keys and the row indexes of the elements in each group as values.\n",
    "* try the `size` method which returns the size of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.6.2: Calculating statistics separately for each group\n",
    "Basic statistical methods can be called on the `DataFrameGroupBy` objects and will be applied on each group separately. You can also use the `apply` method (note that it only handles functions as argument, to get the flexibility discussed above in Ex. 20.5.3 and 20.5.4, you could use the `agg` or `aggregate` method instead, although it only handles functions that aggregate the data, i.e. return a single value per `Serie`s).\n",
    "* Calculate the `min` for every column for each group\n",
    "  * Simply use the `.min` method on `grouped`\n",
    "  * Using the `apply` method\n",
    "* Calculate the `median` for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.6.3 (Supplementary): Iterating over groups\n",
    "One can also iterate over groups in a `DataFrameGroupBy` object.\n",
    "We will test this by making a plot with different colors for the different groups of our `grouped` object.\n",
    "* Make a plot showing `Progression` on the x-axis and `bmi` on the `y-axis`, using a different color for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20.7 (Supplementary): Putting it all together\n",
    "Here a more complex exercise, combining `sklearn`, `pandas`, `numpy` and `matplotlib`\n",
    "* Use `sklearn.cluster.KMeans` to cluster the data according to two features `bmi` and `s3`\n",
    "    * Use 3 clusters (`KMeans(n_clusters=3)`)\n",
    "    * perform the fit using the `fit_predict` method\n",
    "    * This will return an array with an index for each row of `diabetes` corresponding to the cluster number to which that row belongs\n",
    "* Add the cluster indexes as a new feature to `diabetes`\n",
    "* Group the data according to that new feature\n",
    "* Make a plot of `bmi` vs `s3`, using different colors for each cluster (iterate over the groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "* A good short intro into numpy and pandas and how they are intertwined as well as data layout in memory in numpy\n",
    "https://blog.thedataincubator.com/2018/02/numpy-and-pandas/\n",
    "* A short intro on the functionalities in pandas http://pandas.pydata.org/pandas-docs/stable/10min.html\n",
    "* Pandas complete documentation\n",
    "http://pandas.pydata.org/pandas-docs/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
